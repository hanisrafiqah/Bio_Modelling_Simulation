{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1q_qKIV2t2u"
      },
      "source": [
        "#Import all library needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "#confusion matrix visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Link notebook with google drive and access data from your personal Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "### 2.Set the data path for dataset and model location (ex: model_loc = \"/content/gdrive/My Drive/Dataset/\")\n",
        "dataset_dir = \"drive/My Drive/SEM 8/BMS\"#insert the path here\n",
        "model_loc = \"drive/My Drive/SEM 8/BMS\"#insert the path here\n",
        "\n",
        "print(os.listdir(dataset_dir))\n",
        "data = pd.read_csv(\"drive/My Drive/SEM 8/BMS/heart.csv\")"
      ],
      "metadata": {
        "id": "WazdlOZefP88",
        "outputId": "8bc7371f-842c-47ba-aeed-698a2c8dee9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['heart.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZADep6q2t3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3ebe5ec6-a4ca-466a-c44b-a87dfe100e09"
      },
      "source": [
        "### 3. Insert Exploratory data analysis (EDA) steps to analyze and investigate datasets.\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-744438a7-7902-4c6f-a047-c28611a5b9d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-744438a7-7902-4c6f-a047-c28611a5b9d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-744438a7-7902-4c6f-a047-c28611a5b9d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-744438a7-7902-4c6f-a047-c28611a5b9d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-671c441c-b491-458b-acd4-22f746979b97\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-671c441c-b491-458b-acd4-22f746979b97')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-671c441c-b491-458b-acd4-22f746979b97 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 303,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          46,\n          66,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 94,\n        \"max\": 200,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          104,\n          123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 126,\n        \"max\": 564,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          277,\n          169\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22,\n        \"min\": 71,\n        \"max\": 202,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          159,\n          152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1610750220686348,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          1.9,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ylkru32t27"
      },
      "source": [
        "### 4. What is the purpose of the code that sets a list of categorical variables\n",
        "### in a dataset and then casts those variables to the object data type using the astype() function?\n",
        "\n",
        "catagorialList = ['sex','cp','fbs','restecg','exang','ca','thal']\n",
        "for item in catagorialList:\n",
        "    data[item] = data[item].astype('object') #casting to object\n",
        "\n",
        "### By casting these specific columns to the object data type, it allows them to be treated as categorical variables.\n",
        "### This can be useful for various data analysis and modeling tasks, as it enables easier manipulation and interpretation of these variables.\n",
        "### Additionally, some machine learning algorithms require categorical variables to be in this format for proper handling."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNbqP4z32t3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8ae66b-018c-4927-d741-f1b1558e7857"
      },
      "source": [
        " ### 5. Create more data by categorical variable into indicator variables using 'get_dummies' function\n",
        "\n",
        "data = pd.get_dummies(data, drop_first=True)#complete your code here"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n",
            "<ipython-input-8-2eed7fe3153c>:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  data = pd.get_dummies(data, drop_first=True)#complete your code here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhlOEgqg2t3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881cd92b-479f-4cb4-9351-0693840323dc"
      },
      "source": [
        "### 6. Explain line 3,4 and 5 and print the shape of x and y\n",
        "\n",
        "y = data['target'].values\n",
        "y = y.reshape(y.shape[0],1)\n",
        "x = data.drop(['target'],axis=1)\n",
        "y.shape\n",
        "### Line 3 extracts the values from the 'target' column of the DataFrame data and assigns it to the variable y.\n",
        "### Line 4 reshapes the array y. The reshape() function in NumPy is used to change the shape of an array.\n",
        "### Line 5 creates a new DataFrame x by dropping the 'target' column from the original DataFrame data."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEGdOBJu2t3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80948c23-1559-4fef-cdb5-407a1ce0adbf"
      },
      "source": [
        "### 7. Create a simple dataset and demonstrate the normalization code on the simple dataset\n",
        "\n",
        "data = pd.DataFrame({'A': [10, 20, 30], 'B': [100, 200, 300], 'C': [1000, 2000, 3000]})\n",
        "print('Original dataset:')\n",
        "print(data)\n",
        "\n",
        "minx = np.min(data)\n",
        "maxx = np.max(data)\n",
        "data_norm = (data - minx) / (maxx - minx)\n",
        "print('\\nNormalized dataset:')\n",
        "print(data_norm)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset:\n",
            "    A    B     C\n",
            "0  10  100  1000\n",
            "1  20  200  2000\n",
            "2  30  300  3000\n",
            "\n",
            "Normalized dataset:\n",
            "     A    B    C\n",
            "0  0.0  0.0  0.0\n",
            "1  0.5  0.5  0.5\n",
            "2  1.0  1.0  1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 8. Describe the heart dataset after implementing the min max normalization\n",
        "#Normalize data (range 0 - 1)\n",
        "minx = np.min(x)\n",
        "maxx = np.max(x)\n",
        "x = (x - minx) / (maxx - minx)\n",
        "x.head()"
      ],
      "metadata": {
        "id": "asoFBQaumuKA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "430db2b3-815a-4acf-9c28-48f15319ffcc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        age  trestbps      chol   thalach   oldpeak  slope  sex_1  cp_1  cp_2  \\\n",
              "0  0.708333  0.481132  0.244292  0.603053  0.370968    0.0    1.0   0.0   0.0   \n",
              "1  0.166667  0.339623  0.283105  0.885496  0.564516    0.0    1.0   0.0   1.0   \n",
              "2  0.250000  0.339623  0.178082  0.770992  0.225806    1.0    0.0   1.0   0.0   \n",
              "3  0.562500  0.245283  0.251142  0.816794  0.129032    1.0    1.0   1.0   0.0   \n",
              "4  0.583333  0.245283  0.520548  0.702290  0.096774    1.0    0.0   0.0   0.0   \n",
              "\n",
              "   cp_3  ...  restecg_1  restecg_2  exang_1  ca_1  ca_2  ca_3  ca_4  thal_1  \\\n",
              "0   1.0  ...        0.0        0.0      0.0   0.0   0.0   0.0   0.0     1.0   \n",
              "1   0.0  ...        1.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "2   0.0  ...        0.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "3   0.0  ...        1.0        0.0      0.0   0.0   0.0   0.0   0.0     0.0   \n",
              "4   0.0  ...        1.0        0.0      1.0   0.0   0.0   0.0   0.0     0.0   \n",
              "\n",
              "   thal_2  thal_3  \n",
              "0     0.0     0.0  \n",
              "1     1.0     0.0  \n",
              "2     1.0     0.0  \n",
              "3     1.0     0.0  \n",
              "4     1.0     0.0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-626d5ba4-e97d-4f8d-b24d-83f4b67ab8c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>thalach</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>sex_1</th>\n",
              "      <th>cp_1</th>\n",
              "      <th>cp_2</th>\n",
              "      <th>cp_3</th>\n",
              "      <th>...</th>\n",
              "      <th>restecg_1</th>\n",
              "      <th>restecg_2</th>\n",
              "      <th>exang_1</th>\n",
              "      <th>ca_1</th>\n",
              "      <th>ca_2</th>\n",
              "      <th>ca_3</th>\n",
              "      <th>ca_4</th>\n",
              "      <th>thal_1</th>\n",
              "      <th>thal_2</th>\n",
              "      <th>thal_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.481132</td>\n",
              "      <td>0.244292</td>\n",
              "      <td>0.603053</td>\n",
              "      <td>0.370968</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.283105</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.178082</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.251142</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.520548</td>\n",
              "      <td>0.702290</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-626d5ba4-e97d-4f8d-b24d-83f4b67ab8c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-626d5ba4-e97d-4f8d-b24d-83f4b67ab8c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-626d5ba4-e97d-4f8d-b24d-83f4b67ab8c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6cc5e1ad-e6d9-4c5a-b343-e8a47eb8a207\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6cc5e1ad-e6d9-4c5a-b343-e8a47eb8a207')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6cc5e1ad-e6d9-4c5a-b343-e8a47eb8a207 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvykedw82t3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7870d6c7-7a81-4b93-99f4-6cea51288f4f"
      },
      "source": [
        "### 9. Modify the code to split the dataset into train and test (train 70%, val 20% and test 10%).\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "# re-create train and validation set\n",
        "x_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "# train 70%, validation 20%, test 10%\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(217, 21)\n",
            "(55, 21)\n",
            "(31, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pwz5A_j2t30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8fe499-f204-4d18-ea96-3e39cb07b172"
      },
      "source": [
        "### 10. What is the purpose of each layer in the neural network created using the Sequential() function with 64, 32, and 1 neurons,\n",
        "### respectively, and softmax and sigmoid activation functions?\n",
        "\n",
        "model = Sequential() #Allow us to create model layer by layer\n",
        "model.add(Dense(64, input_dim=21, activation='softmax')) #Softmax turn number data into probabilities which sum to 1\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dense(1, activation='sigmoid')) # produce probability value (number between 0 or 1)\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                1408      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3521 (13.75 KB)\n",
            "Trainable params: 3521 (13.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0pM4z_OQfNi"
      },
      "source": [
        "### 11. This code compiles a neural network model with a mean squared error loss function, the Adam optimizer with a learning rate of 0.01,\n",
        "### and accuracy as a performance metric. What does each of these components mean, and how do they affect the model training and performance?\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999,epsilon=1e-07, amsgrad=False,name='Adam'),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unxSIBnZ2t36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f73329-0515-42e3-9824-c7e39f45bd10"
      },
      "source": [
        "# start the model training\n",
        "output = []\n",
        "early = EarlyStopping(monitor='val_acc', patience=400, mode='auto')\n",
        "checkpoint = ModelCheckpoint(model_loc+\"heart_disease_best_model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto', save_freq='epoch')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.01, patience=100, verbose=1, mode='auto', min_lr=0.001)\n",
        "callbacks_list = [early]\n",
        "\n",
        "output = model.fit(x_train, y_train,validation_data=(x_val,y_val), epochs=1000, batch_size=16, verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "14/14 [==============================] - 2s 32ms/step - loss: 0.2490 - acc: 0.5438 - val_loss: 0.2479 - val_acc: 0.5455\n",
            "Epoch 2/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2478 - acc: 0.5438 - val_loss: 0.2460 - val_acc: 0.5455\n",
            "Epoch 3/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2451 - acc: 0.5438 - val_loss: 0.2419 - val_acc: 0.5455\n",
            "Epoch 4/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2397 - acc: 0.5438 - val_loss: 0.2331 - val_acc: 0.5455\n",
            "Epoch 5/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2285 - acc: 0.6498 - val_loss: 0.2167 - val_acc: 0.7455\n",
            "Epoch 6/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2111 - acc: 0.8341 - val_loss: 0.1944 - val_acc: 0.8364\n",
            "Epoch 7/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1889 - acc: 0.8525 - val_loss: 0.1670 - val_acc: 0.9091\n",
            "Epoch 8/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1680 - acc: 0.8525 - val_loss: 0.1515 - val_acc: 0.8727\n",
            "Epoch 9/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1508 - acc: 0.8664 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 10/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1364 - acc: 0.8618 - val_loss: 0.1253 - val_acc: 0.8909\n",
            "Epoch 11/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1284 - acc: 0.8618 - val_loss: 0.1245 - val_acc: 0.8727\n",
            "Epoch 12/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1211 - acc: 0.8710 - val_loss: 0.1304 - val_acc: 0.8182\n",
            "Epoch 13/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1155 - acc: 0.8664 - val_loss: 0.1173 - val_acc: 0.8545\n",
            "Epoch 14/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1102 - acc: 0.8710 - val_loss: 0.1284 - val_acc: 0.8000\n",
            "Epoch 15/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1088 - acc: 0.8664 - val_loss: 0.1294 - val_acc: 0.7818\n",
            "Epoch 16/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1054 - acc: 0.8710 - val_loss: 0.1180 - val_acc: 0.8182\n",
            "Epoch 17/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0989 - acc: 0.8848 - val_loss: 0.1340 - val_acc: 0.8000\n",
            "Epoch 18/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0978 - acc: 0.8848 - val_loss: 0.1194 - val_acc: 0.8182\n",
            "Epoch 19/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0947 - acc: 0.8848 - val_loss: 0.1203 - val_acc: 0.8182\n",
            "Epoch 20/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0909 - acc: 0.8986 - val_loss: 0.1255 - val_acc: 0.8182\n",
            "Epoch 21/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0883 - acc: 0.8986 - val_loss: 0.1097 - val_acc: 0.8364\n",
            "Epoch 22/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0855 - acc: 0.9124 - val_loss: 0.1146 - val_acc: 0.8364\n",
            "Epoch 23/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0852 - acc: 0.9078 - val_loss: 0.1068 - val_acc: 0.8545\n",
            "Epoch 24/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0833 - acc: 0.9078 - val_loss: 0.1121 - val_acc: 0.8545\n",
            "Epoch 25/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0833 - acc: 0.9078 - val_loss: 0.1110 - val_acc: 0.8545\n",
            "Epoch 26/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0808 - acc: 0.9124 - val_loss: 0.1011 - val_acc: 0.8545\n",
            "Epoch 27/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0835 - acc: 0.9032 - val_loss: 0.1146 - val_acc: 0.8545\n",
            "Epoch 28/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0826 - acc: 0.8986 - val_loss: 0.1083 - val_acc: 0.8545\n",
            "Epoch 29/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0785 - acc: 0.9124 - val_loss: 0.1024 - val_acc: 0.8364\n",
            "Epoch 30/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0752 - acc: 0.9217 - val_loss: 0.1126 - val_acc: 0.8545\n",
            "Epoch 31/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0760 - acc: 0.9171 - val_loss: 0.1070 - val_acc: 0.8364\n",
            "Epoch 32/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0747 - acc: 0.9217 - val_loss: 0.1049 - val_acc: 0.8545\n",
            "Epoch 33/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0736 - acc: 0.9263 - val_loss: 0.1045 - val_acc: 0.8545\n",
            "Epoch 34/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0732 - acc: 0.9263 - val_loss: 0.1063 - val_acc: 0.8545\n",
            "Epoch 35/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0720 - acc: 0.9263 - val_loss: 0.1078 - val_acc: 0.8545\n",
            "Epoch 36/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0715 - acc: 0.9263 - val_loss: 0.1116 - val_acc: 0.8545\n",
            "Epoch 37/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0715 - acc: 0.9263 - val_loss: 0.1102 - val_acc: 0.8545\n",
            "Epoch 38/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0706 - acc: 0.9263 - val_loss: 0.1130 - val_acc: 0.8545\n",
            "Epoch 39/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0703 - acc: 0.9263 - val_loss: 0.1112 - val_acc: 0.8545\n",
            "Epoch 40/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0710 - acc: 0.9217 - val_loss: 0.1125 - val_acc: 0.8545\n",
            "Epoch 41/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0701 - acc: 0.9263 - val_loss: 0.1226 - val_acc: 0.8182\n",
            "Epoch 42/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0693 - acc: 0.9263 - val_loss: 0.1164 - val_acc: 0.8545\n",
            "Epoch 43/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0687 - acc: 0.9263 - val_loss: 0.1164 - val_acc: 0.8727\n",
            "Epoch 44/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0681 - acc: 0.9263 - val_loss: 0.1225 - val_acc: 0.8182\n",
            "Epoch 45/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0678 - acc: 0.9263 - val_loss: 0.1224 - val_acc: 0.8182\n",
            "Epoch 46/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0673 - acc: 0.9263 - val_loss: 0.1194 - val_acc: 0.8545\n",
            "Epoch 47/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0670 - acc: 0.9263 - val_loss: 0.1214 - val_acc: 0.8545\n",
            "Epoch 48/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0662 - acc: 0.9309 - val_loss: 0.1251 - val_acc: 0.8545\n",
            "Epoch 49/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0662 - acc: 0.9263 - val_loss: 0.1263 - val_acc: 0.8364\n",
            "Epoch 50/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0661 - acc: 0.9309 - val_loss: 0.1257 - val_acc: 0.8545\n",
            "Epoch 51/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0652 - acc: 0.9309 - val_loss: 0.1220 - val_acc: 0.8545\n",
            "Epoch 52/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0653 - acc: 0.9309 - val_loss: 0.1282 - val_acc: 0.8364\n",
            "Epoch 53/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0645 - acc: 0.9309 - val_loss: 0.1231 - val_acc: 0.8364\n",
            "Epoch 54/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0642 - acc: 0.9309 - val_loss: 0.1307 - val_acc: 0.8364\n",
            "Epoch 55/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0645 - acc: 0.9263 - val_loss: 0.1182 - val_acc: 0.8364\n",
            "Epoch 56/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0651 - acc: 0.9309 - val_loss: 0.1289 - val_acc: 0.8364\n",
            "Epoch 57/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0618 - acc: 0.9309 - val_loss: 0.1194 - val_acc: 0.8545\n",
            "Epoch 58/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0629 - acc: 0.9309 - val_loss: 0.1279 - val_acc: 0.8364\n",
            "Epoch 59/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0618 - acc: 0.9309 - val_loss: 0.1279 - val_acc: 0.8364\n",
            "Epoch 60/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0617 - acc: 0.9309 - val_loss: 0.1279 - val_acc: 0.8364\n",
            "Epoch 61/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0610 - acc: 0.9355 - val_loss: 0.1244 - val_acc: 0.8364\n",
            "Epoch 62/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0604 - acc: 0.9309 - val_loss: 0.1246 - val_acc: 0.8364\n",
            "Epoch 63/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0602 - acc: 0.9309 - val_loss: 0.1259 - val_acc: 0.8364\n",
            "Epoch 64/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0580 - acc: 0.9355 - val_loss: 0.1217 - val_acc: 0.8364\n",
            "Epoch 65/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0571 - acc: 0.9401 - val_loss: 0.1353 - val_acc: 0.8182\n",
            "Epoch 66/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0550 - acc: 0.9401 - val_loss: 0.1254 - val_acc: 0.8364\n",
            "Epoch 67/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0536 - acc: 0.9447 - val_loss: 0.1356 - val_acc: 0.8182\n",
            "Epoch 68/1000\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0486 - acc: 0.9493 - val_loss: 0.1203 - val_acc: 0.8545\n",
            "Epoch 69/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0455 - acc: 0.9585 - val_loss: 0.1228 - val_acc: 0.8364\n",
            "Epoch 70/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0429 - acc: 0.9585 - val_loss: 0.1289 - val_acc: 0.8364\n",
            "Epoch 71/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0417 - acc: 0.9585 - val_loss: 0.1310 - val_acc: 0.8182\n",
            "Epoch 72/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0408 - acc: 0.9585 - val_loss: 0.1370 - val_acc: 0.8182\n",
            "Epoch 73/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0407 - acc: 0.9585 - val_loss: 0.1402 - val_acc: 0.8364\n",
            "Epoch 74/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0400 - acc: 0.9585 - val_loss: 0.1399 - val_acc: 0.8364\n",
            "Epoch 75/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0411 - acc: 0.9585 - val_loss: 0.1469 - val_acc: 0.8364\n",
            "Epoch 76/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0394 - acc: 0.9585 - val_loss: 0.1420 - val_acc: 0.8364\n",
            "Epoch 77/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0385 - acc: 0.9585 - val_loss: 0.1453 - val_acc: 0.8364\n",
            "Epoch 78/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0357 - acc: 0.9631 - val_loss: 0.1414 - val_acc: 0.8182\n",
            "Epoch 79/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0344 - acc: 0.9677 - val_loss: 0.1519 - val_acc: 0.8182\n",
            "Epoch 80/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0341 - acc: 0.9677 - val_loss: 0.1475 - val_acc: 0.8364\n",
            "Epoch 81/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0335 - acc: 0.9677 - val_loss: 0.1545 - val_acc: 0.8182\n",
            "Epoch 82/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0338 - acc: 0.9677 - val_loss: 0.1579 - val_acc: 0.8000\n",
            "Epoch 83/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0329 - acc: 0.9677 - val_loss: 0.1520 - val_acc: 0.8000\n",
            "Epoch 84/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0325 - acc: 0.9677 - val_loss: 0.1539 - val_acc: 0.8000\n",
            "Epoch 85/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0300 - acc: 0.9724 - val_loss: 0.1544 - val_acc: 0.8000\n",
            "Epoch 86/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0301 - acc: 0.9724 - val_loss: 0.1639 - val_acc: 0.8000\n",
            "Epoch 87/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0283 - acc: 0.9724 - val_loss: 0.1617 - val_acc: 0.7818\n",
            "Epoch 88/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0285 - acc: 0.9724 - val_loss: 0.1660 - val_acc: 0.7818\n",
            "Epoch 89/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0284 - acc: 0.9724 - val_loss: 0.1636 - val_acc: 0.7818\n",
            "Epoch 90/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0281 - acc: 0.9724 - val_loss: 0.1662 - val_acc: 0.7818\n",
            "Epoch 91/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0280 - acc: 0.9724 - val_loss: 0.1660 - val_acc: 0.7818\n",
            "Epoch 92/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0284 - acc: 0.9677 - val_loss: 0.1675 - val_acc: 0.7818\n",
            "Epoch 93/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0278 - acc: 0.9724 - val_loss: 0.1733 - val_acc: 0.7818\n",
            "Epoch 94/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0281 - acc: 0.9724 - val_loss: 0.1627 - val_acc: 0.7818\n",
            "Epoch 95/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0277 - acc: 0.9724 - val_loss: 0.1722 - val_acc: 0.8000\n",
            "Epoch 96/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0273 - acc: 0.9724 - val_loss: 0.1692 - val_acc: 0.7818\n",
            "Epoch 97/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0271 - acc: 0.9724 - val_loss: 0.1701 - val_acc: 0.7818\n",
            "Epoch 98/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0271 - acc: 0.9724 - val_loss: 0.1721 - val_acc: 0.7818\n",
            "Epoch 99/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0270 - acc: 0.9724 - val_loss: 0.1729 - val_acc: 0.7818\n",
            "Epoch 100/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0272 - acc: 0.9724 - val_loss: 0.1665 - val_acc: 0.7818\n",
            "Epoch 101/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0267 - acc: 0.9724 - val_loss: 0.1743 - val_acc: 0.7818\n",
            "Epoch 102/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0268 - acc: 0.9724 - val_loss: 0.1686 - val_acc: 0.7818\n",
            "Epoch 103/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0267 - acc: 0.9724 - val_loss: 0.1732 - val_acc: 0.7818\n",
            "Epoch 104/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0253 - acc: 0.9724 - val_loss: 0.1687 - val_acc: 0.7818\n",
            "Epoch 105/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0237 - acc: 0.9770 - val_loss: 0.1698 - val_acc: 0.7818\n",
            "Epoch 106/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0240 - acc: 0.9770 - val_loss: 0.1656 - val_acc: 0.7818\n",
            "Epoch 107/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0240 - acc: 0.9770 - val_loss: 0.1670 - val_acc: 0.7818\n",
            "Epoch 108/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0244 - acc: 0.9724 - val_loss: 0.1629 - val_acc: 0.7818\n",
            "Epoch 109/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0236 - acc: 0.9770 - val_loss: 0.1655 - val_acc: 0.7818\n",
            "Epoch 110/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0231 - acc: 0.9770 - val_loss: 0.1666 - val_acc: 0.7818\n",
            "Epoch 111/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0231 - acc: 0.9770 - val_loss: 0.1660 - val_acc: 0.7818\n",
            "Epoch 112/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0230 - acc: 0.9770 - val_loss: 0.1650 - val_acc: 0.7818\n",
            "Epoch 113/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0229 - acc: 0.9770 - val_loss: 0.1658 - val_acc: 0.7818\n",
            "Epoch 114/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0230 - acc: 0.9770 - val_loss: 0.1689 - val_acc: 0.7818\n",
            "Epoch 115/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0229 - acc: 0.9770 - val_loss: 0.1672 - val_acc: 0.7818\n",
            "Epoch 116/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1668 - val_acc: 0.8000\n",
            "Epoch 117/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1674 - val_acc: 0.7818\n",
            "Epoch 118/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1678 - val_acc: 0.8000\n",
            "Epoch 119/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1653 - val_acc: 0.7818\n",
            "Epoch 120/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1687 - val_acc: 0.7818\n",
            "Epoch 121/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1674 - val_acc: 0.8000\n",
            "Epoch 122/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1685 - val_acc: 0.8000\n",
            "Epoch 123/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1684 - val_acc: 0.8000\n",
            "Epoch 124/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1686 - val_acc: 0.7818\n",
            "Epoch 125/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1687 - val_acc: 0.8000\n",
            "Epoch 126/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1698 - val_acc: 0.8000\n",
            "Epoch 127/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1684 - val_acc: 0.8000\n",
            "Epoch 128/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1694 - val_acc: 0.8000\n",
            "Epoch 129/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1703 - val_acc: 0.8000\n",
            "Epoch 130/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1702 - val_acc: 0.8000\n",
            "Epoch 131/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1696 - val_acc: 0.8000\n",
            "Epoch 132/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1667 - val_acc: 0.8000\n",
            "Epoch 133/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1686 - val_acc: 0.8000\n",
            "Epoch 134/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1694 - val_acc: 0.8000\n",
            "Epoch 135/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1687 - val_acc: 0.8000\n",
            "Epoch 136/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1700 - val_acc: 0.8000\n",
            "Epoch 137/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1685 - val_acc: 0.8000\n",
            "Epoch 138/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1693 - val_acc: 0.8000\n",
            "Epoch 139/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1692 - val_acc: 0.8000\n",
            "Epoch 140/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1685 - val_acc: 0.8000\n",
            "Epoch 141/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1697 - val_acc: 0.8000\n",
            "Epoch 142/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1698 - val_acc: 0.8000\n",
            "Epoch 143/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1692 - val_acc: 0.8000\n",
            "Epoch 144/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1698 - val_acc: 0.8000\n",
            "Epoch 145/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1696 - val_acc: 0.8000\n",
            "Epoch 146/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1693 - val_acc: 0.8000\n",
            "Epoch 147/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1696 - val_acc: 0.8000\n",
            "Epoch 148/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1690 - val_acc: 0.8000\n",
            "Epoch 149/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1688 - val_acc: 0.8000\n",
            "Epoch 150/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1701 - val_acc: 0.8000\n",
            "Epoch 151/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1678 - val_acc: 0.8000\n",
            "Epoch 152/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1688 - val_acc: 0.8000\n",
            "Epoch 153/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1686 - val_acc: 0.8000\n",
            "Epoch 154/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1702 - val_acc: 0.8000\n",
            "Epoch 155/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1697 - val_acc: 0.8000\n",
            "Epoch 156/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1696 - val_acc: 0.8000\n",
            "Epoch 157/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1692 - val_acc: 0.8000\n",
            "Epoch 158/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1684 - val_acc: 0.8000\n",
            "Epoch 159/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1684 - val_acc: 0.8000\n",
            "Epoch 160/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1673 - val_acc: 0.8000\n",
            "Epoch 161/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1667 - val_acc: 0.8000\n",
            "Epoch 162/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1673 - val_acc: 0.8000\n",
            "Epoch 163/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0223 - acc: 0.9770 - val_loss: 0.1671 - val_acc: 0.8000\n",
            "Epoch 164/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1675 - val_acc: 0.8000\n",
            "Epoch 165/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1675 - val_acc: 0.8000\n",
            "Epoch 166/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0221 - acc: 0.9770 - val_loss: 0.1670 - val_acc: 0.8000\n",
            "Epoch 167/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0228 - acc: 0.9770 - val_loss: 0.1722 - val_acc: 0.8000\n",
            "Epoch 168/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0232 - acc: 0.9770 - val_loss: 0.1648 - val_acc: 0.8000\n",
            "Epoch 169/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0230 - acc: 0.9770 - val_loss: 0.1736 - val_acc: 0.8000\n",
            "Epoch 170/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0224 - acc: 0.9770 - val_loss: 0.1700 - val_acc: 0.8000\n",
            "Epoch 171/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1666 - val_acc: 0.8000\n",
            "Epoch 172/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1648 - val_acc: 0.8000\n",
            "Epoch 173/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1623 - val_acc: 0.8182\n",
            "Epoch 174/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0220 - acc: 0.9770 - val_loss: 0.1623 - val_acc: 0.8182\n",
            "Epoch 175/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0217 - acc: 0.9770 - val_loss: 0.1548 - val_acc: 0.8364\n",
            "Epoch 176/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0210 - acc: 0.9724 - val_loss: 0.1769 - val_acc: 0.7818\n",
            "Epoch 177/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0227 - acc: 0.9770 - val_loss: 0.1620 - val_acc: 0.8000\n",
            "Epoch 178/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0232 - acc: 0.9770 - val_loss: 0.1918 - val_acc: 0.7818\n",
            "Epoch 179/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0321 - acc: 0.9585 - val_loss: 0.1498 - val_acc: 0.8182\n",
            "Epoch 180/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0313 - acc: 0.9677 - val_loss: 0.1559 - val_acc: 0.8364\n",
            "Epoch 181/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0296 - acc: 0.9631 - val_loss: 0.1663 - val_acc: 0.8000\n",
            "Epoch 182/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0236 - acc: 0.9770 - val_loss: 0.1726 - val_acc: 0.8000\n",
            "Epoch 183/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0229 - acc: 0.9770 - val_loss: 0.1782 - val_acc: 0.7818\n",
            "Epoch 184/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - acc: 0.9770 - val_loss: 0.1845 - val_acc: 0.7818\n",
            "Epoch 185/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0221 - acc: 0.9770 - val_loss: 0.1804 - val_acc: 0.7818\n",
            "Epoch 186/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0220 - acc: 0.9770 - val_loss: 0.1755 - val_acc: 0.7818\n",
            "Epoch 187/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0218 - acc: 0.9770 - val_loss: 0.1669 - val_acc: 0.8000\n",
            "Epoch 188/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0212 - acc: 0.9770 - val_loss: 0.1558 - val_acc: 0.8182\n",
            "Epoch 189/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0205 - acc: 0.9770 - val_loss: 0.1550 - val_acc: 0.8182\n",
            "Epoch 190/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0205 - acc: 0.9770 - val_loss: 0.1709 - val_acc: 0.8000\n",
            "Epoch 191/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0211 - acc: 0.9770 - val_loss: 0.1613 - val_acc: 0.8000\n",
            "Epoch 192/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0212 - acc: 0.9770 - val_loss: 0.1648 - val_acc: 0.8000\n",
            "Epoch 193/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0186 - acc: 0.9816 - val_loss: 0.1612 - val_acc: 0.8000\n",
            "Epoch 194/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0184 - acc: 0.9816 - val_loss: 0.1636 - val_acc: 0.8000\n",
            "Epoch 195/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.1667 - val_acc: 0.8000\n",
            "Epoch 196/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.1681 - val_acc: 0.8000\n",
            "Epoch 197/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.1664 - val_acc: 0.8000\n",
            "Epoch 198/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.1686 - val_acc: 0.8000\n",
            "Epoch 199/1000\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.1688 - val_acc: 0.8000\n",
            "Epoch 200/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0182 - acc: 0.9816 - val_loss: 0.1698 - val_acc: 0.8000\n",
            "Epoch 201/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1698 - val_acc: 0.8000\n",
            "Epoch 202/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1703 - val_acc: 0.8000\n",
            "Epoch 203/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1706 - val_acc: 0.8000\n",
            "Epoch 204/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1709 - val_acc: 0.8000\n",
            "Epoch 205/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1714 - val_acc: 0.8000\n",
            "Epoch 206/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1715 - val_acc: 0.8000\n",
            "Epoch 207/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1714 - val_acc: 0.8000\n",
            "Epoch 208/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1720 - val_acc: 0.8000\n",
            "Epoch 209/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1722 - val_acc: 0.8000\n",
            "Epoch 210/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1725 - val_acc: 0.8000\n",
            "Epoch 211/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1726 - val_acc: 0.8000\n",
            "Epoch 212/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1728 - val_acc: 0.8000\n",
            "Epoch 213/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1732 - val_acc: 0.8000\n",
            "Epoch 214/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1730 - val_acc: 0.8000\n",
            "Epoch 215/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1728 - val_acc: 0.8000\n",
            "Epoch 216/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1734 - val_acc: 0.8000\n",
            "Epoch 217/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1737 - val_acc: 0.8000\n",
            "Epoch 218/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1734 - val_acc: 0.8000\n",
            "Epoch 219/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1737 - val_acc: 0.8000\n",
            "Epoch 220/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1737 - val_acc: 0.8000\n",
            "Epoch 221/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1739 - val_acc: 0.8000\n",
            "Epoch 222/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1739 - val_acc: 0.8000\n",
            "Epoch 223/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1748 - val_acc: 0.8000\n",
            "Epoch 224/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1750 - val_acc: 0.8000\n",
            "Epoch 225/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 226/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 227/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1749 - val_acc: 0.8000\n",
            "Epoch 228/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1752 - val_acc: 0.8000\n",
            "Epoch 229/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 230/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1749 - val_acc: 0.8000\n",
            "Epoch 231/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1745 - val_acc: 0.8000\n",
            "Epoch 232/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1744 - val_acc: 0.8000\n",
            "Epoch 233/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 234/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 235/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 236/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1745 - val_acc: 0.8000\n",
            "Epoch 237/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 238/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1749 - val_acc: 0.8000\n",
            "Epoch 239/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1752 - val_acc: 0.8000\n",
            "Epoch 240/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1752 - val_acc: 0.8000\n",
            "Epoch 241/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1751 - val_acc: 0.8000\n",
            "Epoch 242/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1752 - val_acc: 0.8000\n",
            "Epoch 243/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1748 - val_acc: 0.8000\n",
            "Epoch 244/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1751 - val_acc: 0.8000\n",
            "Epoch 245/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1751 - val_acc: 0.8000\n",
            "Epoch 246/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1742 - val_acc: 0.8000\n",
            "Epoch 247/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1740 - val_acc: 0.8000\n",
            "Epoch 248/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1736 - val_acc: 0.8000\n",
            "Epoch 249/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1749 - val_acc: 0.8000\n",
            "Epoch 250/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1743 - val_acc: 0.8000\n",
            "Epoch 251/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1733 - val_acc: 0.8000\n",
            "Epoch 252/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1762 - val_acc: 0.8000\n",
            "Epoch 253/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1764 - val_acc: 0.8000\n",
            "Epoch 254/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1756 - val_acc: 0.8000\n",
            "Epoch 255/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1759 - val_acc: 0.8000\n",
            "Epoch 256/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1743 - val_acc: 0.8000\n",
            "Epoch 257/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0208 - acc: 0.9770 - val_loss: 0.1790 - val_acc: 0.8000\n",
            "Epoch 258/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1846 - val_acc: 0.8000\n",
            "Epoch 259/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0185 - acc: 0.9816 - val_loss: 0.1824 - val_acc: 0.8000\n",
            "Epoch 260/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0183 - acc: 0.9816 - val_loss: 0.1787 - val_acc: 0.8000\n",
            "Epoch 261/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1846 - val_acc: 0.8000\n",
            "Epoch 262/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0180 - acc: 0.9816 - val_loss: 0.1829 - val_acc: 0.8000\n",
            "Epoch 263/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1796 - val_acc: 0.8000\n",
            "Epoch 264/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1802 - val_acc: 0.8000\n",
            "Epoch 265/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1803 - val_acc: 0.8000\n",
            "Epoch 266/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1799 - val_acc: 0.8000\n",
            "Epoch 267/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1797 - val_acc: 0.8000\n",
            "Epoch 268/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1785 - val_acc: 0.8000\n",
            "Epoch 269/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1765 - val_acc: 0.8000\n",
            "Epoch 270/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1752 - val_acc: 0.8000\n",
            "Epoch 271/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1771 - val_acc: 0.8000\n",
            "Epoch 272/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1746 - val_acc: 0.8000\n",
            "Epoch 273/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0216 - acc: 0.9770 - val_loss: 0.1765 - val_acc: 0.7818\n",
            "Epoch 274/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0181 - acc: 0.9816 - val_loss: 0.1829 - val_acc: 0.8000\n",
            "Epoch 275/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1826 - val_acc: 0.8000\n",
            "Epoch 276/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1831 - val_acc: 0.8000\n",
            "Epoch 277/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0179 - acc: 0.9816 - val_loss: 0.1831 - val_acc: 0.8000\n",
            "Epoch 278/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1823 - val_acc: 0.8000\n",
            "Epoch 279/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1820 - val_acc: 0.8000\n",
            "Epoch 280/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1816 - val_acc: 0.8000\n",
            "Epoch 281/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1816 - val_acc: 0.8000\n",
            "Epoch 282/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1812 - val_acc: 0.8000\n",
            "Epoch 283/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1805 - val_acc: 0.8000\n",
            "Epoch 284/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1807 - val_acc: 0.8000\n",
            "Epoch 285/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1807 - val_acc: 0.8000\n",
            "Epoch 286/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1805 - val_acc: 0.8000\n",
            "Epoch 287/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1805 - val_acc: 0.7818\n",
            "Epoch 288/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1809 - val_acc: 0.7818\n",
            "Epoch 289/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1795 - val_acc: 0.7818\n",
            "Epoch 290/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1781 - val_acc: 0.7818\n",
            "Epoch 291/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1783 - val_acc: 0.7818\n",
            "Epoch 292/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1784 - val_acc: 0.7818\n",
            "Epoch 293/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1781 - val_acc: 0.7818\n",
            "Epoch 294/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1785 - val_acc: 0.7818\n",
            "Epoch 295/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1777 - val_acc: 0.7818\n",
            "Epoch 296/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1777 - val_acc: 0.7818\n",
            "Epoch 297/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1779 - val_acc: 0.7818\n",
            "Epoch 298/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0176 - acc: 0.9816 - val_loss: 0.1788 - val_acc: 0.7818\n",
            "Epoch 299/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1785 - val_acc: 0.7818\n",
            "Epoch 300/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1781 - val_acc: 0.7818\n",
            "Epoch 301/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1780 - val_acc: 0.7818\n",
            "Epoch 302/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1784 - val_acc: 0.8000\n",
            "Epoch 303/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0175 - acc: 0.9816 - val_loss: 0.1778 - val_acc: 0.8000\n",
            "Epoch 304/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1774 - val_acc: 0.8000\n",
            "Epoch 305/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1771 - val_acc: 0.8000\n",
            "Epoch 306/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1776 - val_acc: 0.8000\n",
            "Epoch 307/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1773 - val_acc: 0.8000\n",
            "Epoch 308/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1771 - val_acc: 0.8000\n",
            "Epoch 309/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1772 - val_acc: 0.8000\n",
            "Epoch 310/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1776 - val_acc: 0.8000\n",
            "Epoch 311/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0174 - acc: 0.9816 - val_loss: 0.1777 - val_acc: 0.8000\n",
            "Epoch 312/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0173 - acc: 0.9816 - val_loss: 0.1773 - val_acc: 0.8000\n",
            "Epoch 313/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0173 - acc: 0.9816 - val_loss: 0.1770 - val_acc: 0.8000\n",
            "Epoch 314/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0173 - acc: 0.9816 - val_loss: 0.1773 - val_acc: 0.8000\n",
            "Epoch 315/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0172 - acc: 0.9816 - val_loss: 0.1767 - val_acc: 0.8000\n",
            "Epoch 316/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0172 - acc: 0.9816 - val_loss: 0.1768 - val_acc: 0.8000\n",
            "Epoch 317/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0172 - acc: 0.9816 - val_loss: 0.1758 - val_acc: 0.8000\n",
            "Epoch 318/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0171 - acc: 0.9816 - val_loss: 0.1740 - val_acc: 0.8000\n",
            "Epoch 319/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0172 - acc: 0.9816 - val_loss: 0.1719 - val_acc: 0.8000\n",
            "Epoch 320/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0171 - acc: 0.9816 - val_loss: 0.1704 - val_acc: 0.8000\n",
            "Epoch 321/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0170 - acc: 0.9816 - val_loss: 0.1672 - val_acc: 0.8000\n",
            "Epoch 322/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0169 - acc: 0.9816 - val_loss: 0.1683 - val_acc: 0.8000\n",
            "Epoch 323/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0168 - acc: 0.9816 - val_loss: 0.1613 - val_acc: 0.8364\n",
            "Epoch 324/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0169 - acc: 0.9816 - val_loss: 0.1619 - val_acc: 0.8364\n",
            "Epoch 325/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0167 - acc: 0.9816 - val_loss: 0.1727 - val_acc: 0.7818\n",
            "Epoch 326/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0160 - acc: 0.9816 - val_loss: 0.1649 - val_acc: 0.8182\n",
            "Epoch 327/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0231 - acc: 0.9724 - val_loss: 0.1833 - val_acc: 0.7818\n",
            "Epoch 328/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0177 - acc: 0.9816 - val_loss: 0.1783 - val_acc: 0.8000\n",
            "Epoch 329/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0230 - acc: 0.9770 - val_loss: 0.1710 - val_acc: 0.8182\n",
            "Epoch 330/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0232 - acc: 0.9770 - val_loss: 0.1530 - val_acc: 0.8364\n",
            "Epoch 331/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0260 - acc: 0.9724 - val_loss: 0.1887 - val_acc: 0.7818\n",
            "Epoch 332/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0238 - acc: 0.9724 - val_loss: 0.1624 - val_acc: 0.8182\n",
            "Epoch 333/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0225 - acc: 0.9770 - val_loss: 0.1712 - val_acc: 0.8182\n",
            "Epoch 334/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0226 - acc: 0.9770 - val_loss: 0.1206 - val_acc: 0.8545\n",
            "Epoch 335/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0220 - acc: 0.9770 - val_loss: 0.1196 - val_acc: 0.8727\n",
            "Epoch 336/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0215 - acc: 0.9770 - val_loss: 0.1282 - val_acc: 0.8727\n",
            "Epoch 337/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1409 - val_acc: 0.8364\n",
            "Epoch 338/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0219 - acc: 0.9770 - val_loss: 0.1266 - val_acc: 0.8545\n",
            "Epoch 339/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0191 - acc: 0.9770 - val_loss: 0.1490 - val_acc: 0.8364\n",
            "Epoch 340/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0273 - acc: 0.9677 - val_loss: 0.1561 - val_acc: 0.8364\n",
            "Epoch 341/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0243 - acc: 0.9724 - val_loss: 0.1452 - val_acc: 0.8545\n",
            "Epoch 342/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0227 - acc: 0.9724 - val_loss: 0.1536 - val_acc: 0.8364\n",
            "Epoch 343/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0187 - acc: 0.9770 - val_loss: 0.1572 - val_acc: 0.8182\n",
            "Epoch 344/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0178 - acc: 0.9816 - val_loss: 0.1544 - val_acc: 0.8182\n",
            "Epoch 345/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0169 - acc: 0.9816 - val_loss: 0.1419 - val_acc: 0.8545\n",
            "Epoch 346/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0169 - acc: 0.9816 - val_loss: 0.1438 - val_acc: 0.8364\n",
            "Epoch 347/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0167 - acc: 0.9816 - val_loss: 0.1459 - val_acc: 0.8364\n",
            "Epoch 348/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0167 - acc: 0.9816 - val_loss: 0.1483 - val_acc: 0.8364\n",
            "Epoch 349/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0166 - acc: 0.9816 - val_loss: 0.1501 - val_acc: 0.8364\n",
            "Epoch 350/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0163 - acc: 0.9816 - val_loss: 0.1506 - val_acc: 0.8364\n",
            "Epoch 351/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0164 - acc: 0.9816 - val_loss: 0.1510 - val_acc: 0.8364\n",
            "Epoch 352/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0157 - acc: 0.9816 - val_loss: 0.1525 - val_acc: 0.8364\n",
            "Epoch 353/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0171 - acc: 0.9816 - val_loss: 0.1528 - val_acc: 0.8364\n",
            "Epoch 354/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0157 - acc: 0.9816 - val_loss: 0.1555 - val_acc: 0.8364\n",
            "Epoch 355/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0162 - acc: 0.9816 - val_loss: 0.1568 - val_acc: 0.8364\n",
            "Epoch 356/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0154 - acc: 0.9862 - val_loss: 0.1569 - val_acc: 0.8364\n",
            "Epoch 357/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0162 - acc: 0.9816 - val_loss: 0.1570 - val_acc: 0.8364\n",
            "Epoch 358/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0162 - acc: 0.9816 - val_loss: 0.1532 - val_acc: 0.8364\n",
            "Epoch 359/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0158 - acc: 0.9816 - val_loss: 0.1514 - val_acc: 0.8364\n",
            "Epoch 360/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0150 - acc: 0.9816 - val_loss: 0.1495 - val_acc: 0.8364\n",
            "Epoch 361/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0163 - acc: 0.9770 - val_loss: 0.1524 - val_acc: 0.8364\n",
            "Epoch 362/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0164 - acc: 0.9816 - val_loss: 0.1569 - val_acc: 0.8364\n",
            "Epoch 363/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0165 - acc: 0.9816 - val_loss: 0.1567 - val_acc: 0.8364\n",
            "Epoch 364/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0165 - acc: 0.9816 - val_loss: 0.1542 - val_acc: 0.8364\n",
            "Epoch 365/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0165 - acc: 0.9816 - val_loss: 0.1526 - val_acc: 0.8364\n",
            "Epoch 366/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0164 - acc: 0.9816 - val_loss: 0.1508 - val_acc: 0.8364\n",
            "Epoch 367/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0163 - acc: 0.9816 - val_loss: 0.1495 - val_acc: 0.8364\n",
            "Epoch 368/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0162 - acc: 0.9816 - val_loss: 0.1489 - val_acc: 0.8364\n",
            "Epoch 369/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0161 - acc: 0.9816 - val_loss: 0.1484 - val_acc: 0.8364\n",
            "Epoch 370/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0159 - acc: 0.9816 - val_loss: 0.1480 - val_acc: 0.8364\n",
            "Epoch 371/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0157 - acc: 0.9816 - val_loss: 0.1477 - val_acc: 0.8364\n",
            "Epoch 372/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0151 - acc: 0.9816 - val_loss: 0.1482 - val_acc: 0.8364\n",
            "Epoch 373/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0145 - acc: 0.9862 - val_loss: 0.1497 - val_acc: 0.8364\n",
            "Epoch 374/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0146 - acc: 0.9862 - val_loss: 0.1501 - val_acc: 0.8364\n",
            "Epoch 375/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0146 - acc: 0.9862 - val_loss: 0.1493 - val_acc: 0.8364\n",
            "Epoch 376/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0143 - acc: 0.9862 - val_loss: 0.1502 - val_acc: 0.8364\n",
            "Epoch 377/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0147 - acc: 0.9862 - val_loss: 0.1489 - val_acc: 0.8364\n",
            "Epoch 378/1000\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0152 - acc: 0.9816 - val_loss: 0.1475 - val_acc: 0.8364\n",
            "Epoch 379/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0149 - acc: 0.9816 - val_loss: 0.1443 - val_acc: 0.8545\n",
            "Epoch 380/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0150 - acc: 0.9816 - val_loss: 0.1467 - val_acc: 0.8545\n",
            "Epoch 381/1000\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0162 - acc: 0.9816 - val_loss: 0.1474 - val_acc: 0.8545\n",
            "Epoch 382/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0159 - acc: 0.9816 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 383/1000\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0158 - acc: 0.9816 - val_loss: 0.1437 - val_acc: 0.8545\n",
            "Epoch 384/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0154 - acc: 0.9816 - val_loss: 0.1436 - val_acc: 0.8545\n",
            "Epoch 385/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0141 - acc: 0.9862 - val_loss: 0.1434 - val_acc: 0.8545\n",
            "Epoch 386/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0138 - acc: 0.9862 - val_loss: 0.1434 - val_acc: 0.8545\n",
            "Epoch 387/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0136 - acc: 0.9862 - val_loss: 0.1436 - val_acc: 0.8545\n",
            "Epoch 388/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0140 - acc: 0.9862 - val_loss: 0.1436 - val_acc: 0.8545\n",
            "Epoch 389/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0140 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 390/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0140 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 391/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.1439 - val_acc: 0.8545\n",
            "Epoch 392/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0133 - acc: 0.9862 - val_loss: 0.1441 - val_acc: 0.8545\n",
            "Epoch 393/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 394/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0132 - acc: 0.9862 - val_loss: 0.1439 - val_acc: 0.8545\n",
            "Epoch 395/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0131 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 396/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0131 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 397/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.1439 - val_acc: 0.8545\n",
            "Epoch 398/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0129 - acc: 0.9862 - val_loss: 0.1439 - val_acc: 0.8545\n",
            "Epoch 399/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0130 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 400/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0130 - acc: 0.9862 - val_loss: 0.1437 - val_acc: 0.8545\n",
            "Epoch 401/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0128 - acc: 0.9862 - val_loss: 0.1437 - val_acc: 0.8545\n",
            "Epoch 402/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0128 - acc: 0.9862 - val_loss: 0.1440 - val_acc: 0.8545\n",
            "Epoch 403/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0129 - acc: 0.9862 - val_loss: 0.1437 - val_acc: 0.8545\n",
            "Epoch 404/1000\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0130 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 405/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0125 - acc: 0.9862 - val_loss: 0.1438 - val_acc: 0.8545\n",
            "Epoch 406/1000\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0126 - acc: 0.9862 - val_loss: 0.1440 - val_acc: 0.8545\n",
            "Epoch 407/1000\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0125 - acc: 0.9862 - val_loss: 0.1440 - val_acc: 0.8545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sYpy54d2t4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "0462ad3b-fdb3-4b49-8ab2-6f9ab4dadca3"
      },
      "source": [
        "### 12. What does the plot generated by this code represent?\n",
        "\n",
        "plt.plot(output.history['acc'])\n",
        "plt.plot(output.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "#plt.savefig('Accuracy.png',dpi=100) #to save the image\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIrcCZ8P2t4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c32d312-8437-4cc2-fd48-1a7c1caeebf4"
      },
      "source": [
        "### 13. Plot code for the model loss. You can refer to the plot code for model accuracy above.\n",
        "#model.load_weights(model_loc+\"heart_disease_best_model.hdf5\")\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]))\n",
        "print(\"loss:\", round(scores[0],2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1925 - acc: 0.8065\n",
            "\n",
            "acc: 80.65%\n",
            "loss: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KTWzuJM12t4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca1a24b-a8e8-4ea6-c9b9-b54aaaa33bb1"
      },
      "source": [
        "### 14. What is the purpose of evaluating the model on the test dataset?\n",
        "### Evaluating a model on a test dataset is a critical step in the machine learning workflow\n",
        "### that helps ensure the model's generalization ability, estimate its performance, compare different models, identify issues, and validate assumptions.\n",
        "\n",
        "#model.load_weights(model_loc+\"heart_disease_best_model.hdf5\")\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]))\n",
        "print(\"loss:\", round(scores[0],2))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1925 - acc: 0.8065\n",
            "\n",
            "acc: 80.65%\n",
            "loss: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNYy0CRt2t4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d997bdc-7443-46db-fe48-a9bee63000cf"
      },
      "source": [
        "#Display detailed prediction\n",
        "pred = model.predict(x_test)\n",
        "y = np.round(pred).astype(\"int16\")\n",
        "idx = 0\n",
        "ps = 0\n",
        "fl = 0\n",
        "for x in pred:\n",
        "    if y_test[idx]==y[idx]:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\"Result: \\033[92mPass\")\n",
        "        ps = ps+1\n",
        "    else:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\" Result: \\033[91mFail\")\n",
        "        fl = fl+1\n",
        "    idx = idx + 1\n",
        "print(\"\\033[30mRight Prediction :\",ps, \"Wrong Prediction :\",fl)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 90ms/step\n",
            "\u001b[30mNo: 1 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 2 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 3 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 4 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 5 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 6 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 7 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 8 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 9 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 10 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 11 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 12 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 13 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 14 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 15 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 16 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 17 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 18 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 19 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 20 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 21 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 22 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 23 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 24 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 25 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 26 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 27 Actual: [0]  Predicted: [1]  Result: \u001b[91mFail\n",
            "\u001b[30mNo: 28 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 29 Actual: [0]  Predicted: [0] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 30 Actual: [1]  Predicted: [1] Result: \u001b[92mPass\n",
            "\u001b[30mNo: 31 Actual: [1]  Predicted: [0]  Result: \u001b[91mFail\n",
            "\u001b[30mRight Prediction : 25 Wrong Prediction : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHQBXNX5aYcn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "1cbfce5a-af49-4ccf-bdfd-7383a838c345"
      },
      "source": [
        "### 15. What is Confusion Matrix and why you need it? Explain TP, FP, FN, TN.\n",
        "### A confusion matrix is a table that is often used to describe the performance of a classification model\n",
        "### on a set of test data for which the true values are known. It allows visualization of the performance of an algorithm, typically a supervised learning algorithm.\n",
        "### TP (True Positive), FP (False Positive), FN (False Negative), TN (true Negative)\n",
        "\n",
        "### 16. Explain the classification report produce.\n",
        "### A classification report is a textual summary of various evaluation metrics for a classification model. It provides a comprehensive overview of the model's performance on a per-class basis.\n",
        "### Typically, classification reports are generated after evaluating a model on a test dataset where the true class labels are known.\n",
        "\n",
        "y_pred = y\n",
        "y_true = y_test\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "#cm = confusion_matrix(y_true, y_pred, labels=labels.astype('int'))\n",
        "f, ax=plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cm,annot=True,linewidths=1.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
        "plt.xlabel(\"y_pred\")\n",
        "plt.ylabel(\"y_true\")\n",
        "plt.show()\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, labels=[0,1]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79        14\n",
            "           1       0.82      0.82      0.82        17\n",
            "\n",
            "    accuracy                           0.81        31\n",
            "   macro avg       0.80      0.80      0.80        31\n",
            "weighted avg       0.81      0.81      0.81        31\n",
            "\n"
          ]
        }
      ]
    }
  ]
}